# ğŸ’¡ Chatbot Study
Open Domain ì±—ë´‡ êµ¬í˜„ì„ ìœ„í•œ ìŠ¤í„°ë”” ë‚´ìš© ê¸°ë¡ ì €ì¥ì†Œ

- NLPë¥¼ ê³µë¶€í•˜ê³ ì í•˜ëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ ìë£Œ ì œê³µ
    - ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬, í•™ìŠµ, ì œê³µê¹Œì§€ ì „ë°˜ì ì¸ ê³¼ì •ì„ **ì¢‹ì€ ìë£Œ**ì™€ í•¨ê»˜ ê²½í—˜
    - ì‚½ì§ˆí•œ ê²½í—˜ì„ ìƒì„¸í•˜ê¸° ê¸°ë¡í•œë‹¤

## 1. Bert
#### êµ¬ì¡°
<p align="center">
  <img src="https://user-images.githubusercontent.com/53163222/135712814-34333b78-24d1-42b9-8811-56931720edcc.png">
  <img src="https://user-images.githubusercontent.com/53163222/135712806-4b064e04-e560-4768-99f1-a9b52fb926b5.png">
</p>

#### íŠ¹ì§•
#### ì¥ì 
#### ë‹¨ì 

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

## 2. Seq2Seq
#### êµ¬ì¡°
<p align="center">
  <img width="80%" src="https://user-images.githubusercontent.com/53163222/135714612-e8e4bdcd-e981-4ed8-817b-be0b4fe836c0.png">
</p>

#### íŠ¹ì§•
#### ì¥ì 
#### ë‹¨ì 

[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)