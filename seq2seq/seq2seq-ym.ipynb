{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c00347",
   "metadata": {},
   "source": [
    "# PyTorch Seq2Seq 챗봇 구현 및 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92206e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import re\n",
    "import core.config as conf\n",
    "from konlpy.tag import Mecab  # tweepy오류로 konlpy 직접 설치 필요, mecab별도 설치 readme 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d498218",
   "metadata": {},
   "source": [
    "가상 환경 생성<br> \n",
    "`conda create -n 가상환경이름 python=3.7`\n",
    "\n",
    "PyTorch(Windows, Conda, CUDA 10.2)<br>\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a57ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.legacy.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c11ba",
   "metadata": {},
   "source": [
    "PyTorch에서 GPU가 사용가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93c2a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # GPU 사용 여부\n",
    "torch.cuda.get_device_name(0) # GPU 정보Z\n",
    "torch.cuda.device_count() # 사용가능 GPU 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0 # Device Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3241bb",
   "metadata": {},
   "source": [
    "# 데이터 전처리를 위한 설정\n",
    "Seq2Seq에서의 임베딩은 아래와 같이 추가 토큰을 사용하여 동작을 제어한다.\n",
    "\n",
    "- `<PAD>`: 0, Padding, 짧은 문장을 채울 때 사용하는 토큰\n",
    "- `<SOS>`: 1, Start of Sentence, 문장의 시작을 나타내는 토큰\n",
    "- `<EOS>`: 2, End of Sentence, 문장의 끝을 나타내는 토큰\n",
    "- `<UNK>`: 3, Unkown Words, 없는 단어를 나타내는 토큰\n",
    "\n",
    "디코더 입력에 <SOS>가 들어가면 디코딩(문장)의 시작을 의미하고 출력에 <EOS>가 나오면 디코딩(문장)을 종료한다.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "337fb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "pad = \"<PAD>\"# 패딩\n",
    "sos = \"<SOS>\"# 시작\n",
    "eos = \"<EOS>\"# 끝\n",
    "unk = \"<UNK>\"# Unkown word\n",
    "\n",
    "# 태그 인덱스\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402221ec",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6ba34c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = conf.data_path\n",
    "data_df = pd.read_csv(f'{path}'+'ChatbotData.csv', encoding='utf-8')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f6ad0c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c6e96f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, answer = list(data_df['Q']), list(data_df['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae3290",
   "metadata": {},
   "source": [
    "# 형태소분석\n",
    "\n",
    "챗봇 데이터 문장을 먼저 최소단위로 tokenizing 해야한다. 한국어는 보통 KoNLPy를 사용한다.\n",
    "\n",
    "해당 예시에서는 mecab 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "416c9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\") # 정규 표현식 필터\n",
    "\n",
    "def pos_tag(sentences):\n",
    "    mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\") # KoNLPy mecab 사용\n",
    "    # 일반적인 Konlpy의 토크나이저와는 달리, mecab은 dicpath 파라미터를 지정 필요\n",
    "    \n",
    "    sentences_pos = []     # 문장 품사 변수 초기화\n",
    "    # 모든 문장 반복\n",
    "    for s in sentences:\n",
    "        s = re.sub(RE_FILTER, \"\", s)  # 특수기호 제거\n",
    "        s = \" \".join(mecab.morphs(s))  # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "        sentences_pos.append(s)\n",
    "        \n",
    "    return sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "47153633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소분석 수행\n",
    "question = pos_tag(question)\n",
    "answer = pos_tag(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "177ea360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : 12 시 땡 \n",
      "A: 하루 가 또 가 네요\n",
      "\n",
      "Q : 1 지망 학교 떨어졌 어 \n",
      "A: 위로 해 드립니다\n",
      "\n",
      "Q : 3 박 4 일 놀 러 가 고 싶 다 \n",
      "A: 여행 은 언제나 좋 죠\n",
      "\n",
      "Q : 3 박 4 일 정도 놀 러 가 고 싶 다 \n",
      "A: 여행 은 언제나 좋 죠\n",
      "\n",
      "Q : PPL 심하 네 \n",
      "A: 눈살 이 찌푸려 지 죠\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 형태소분석을 수행한 데이터 출력\n",
    "for i in range(5):\n",
    "    print('Q : %s \\nA: %s\\n'%(question[i], answer[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5a8ab",
   "metadata": {},
   "source": [
    "질문과 대답 문장들을 모두 합쳐서 전체 단어 사전을 만든다. 단어를 인덱스에 따라 정리를 해야 단어와 인덱스간의 전환이 용이하다.\n",
    "\n",
    "- 문장 → 인덱스 배열 임베딩 레이어의 입력으로 사용\n",
    "- 모델의 출력(인덱스 배열) → 문장\n",
    "\n",
    "\n",
    "### 딕셔너리 생성\n",
    "단어 → 인덱스: 모델 입력으로는 인덱스가 필요, 문장을 인덱스로 변환할 때 사용가능<br>\n",
    "인덱스 → 단어: 모델의 예측 결과는 인덱스로 출력, 인덱스를 문장으로 변환할 때  사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "61a64c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = question + answer # 질문과 대답 문장들을 하나로 합침\n",
    "vocab = []\n",
    "\n",
    "for s in sentences:\n",
    "    for word in s.split():\n",
    "        if len(word) > 0: # 길이가 0인 단어는 제외\n",
    "            vocab.append(word) # 전체 단어로 딕셔너리 생성\n",
    "            \n",
    "vocab = list(set(words)) # 중복 단어 삭제\n",
    "vocab[:0] = [pad, sos, eos, unk] # 제일 앞에 태그 단어 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "79ec1f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6809"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) # vocab 단어개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "924651da",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(vocab)} # 단어 → 인덱스\n",
    "index_to_word = {index: word for index, word in enumerate(vocab)} # 인덱스 → 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6cd5cdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 1331,\n",
       " '<SOS>': 1138,\n",
       " '<EOS>': 3098,\n",
       " '<UNK>': 3,\n",
       " '여친': 4,\n",
       " '상종': 5,\n",
       " '믿음': 6,\n",
       " '물론': 7,\n",
       " '예의': 8,\n",
       " '힘드실': 9}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(word_to_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0930577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<SOS>',\n",
       " 2: '<EOS>',\n",
       " 3: '<UNK>',\n",
       " 4: '여친',\n",
       " 5: '상종',\n",
       " 6: '믿음',\n",
       " 7: '물론',\n",
       " 8: '예의',\n",
       " 9: '힘드실'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(index_to_word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bbca43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "def Sent2Idx(sentences, vocabulary, type): \n",
    "    result = []\n",
    "    \n",
    "    for sentence in sentences:     # 모든 문장에 대해서 반복\n",
    "        sentence_index = []\n",
    "        \n",
    "        # 디코더 입력일 경우 맨 앞에 SOS 태그 추가\n",
    "        if type == DECODER_INPUT:\n",
    "            result.extend([vocabulary[sos]])\n",
    "        \n",
    "        # 문장의 단어들을 띄어쓰기로 분리\n",
    "        for word in sentence.split():\n",
    "            if vocabulary.get(word) is not None:\n",
    "                sentence_index.extend([vocabulary[word]]) # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "            else:\n",
    "                sentence_index.extend([vocabulary[unk]]) # 사전에 없는 단어면 UNK 인덱스를 추가\n",
    "\n",
    "        # 최대 길이 검사\n",
    "        if type == DECODER_TARGET:\n",
    "            # 디코더 목표일 경우 맨 뒤에 eos 태그 추가\n",
    "            if len(sentence_index) >= max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[eos]]\n",
    "            else:\n",
    "                sentence_index += [vocabulary[eos]]\n",
    "        else:\n",
    "            if len(sentence_index) > max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences]\n",
    "            \n",
    "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[pad]]         # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "        result.append(sentence_index)         # 문장의 인덱스 배열을 추가\n",
    "\n",
    "    return np.asarray(sentences_index)\n",
    "\n",
    "# 인덱스를 문장으로 변환\n",
    "def Idx2Sent(indexs, vocab): \n",
    "    sentence = ''\n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == EOS_IDX:\n",
    "            break; # 종료 인덱스면 중지\n",
    "        if vocabulary.get(index) is not None:\n",
    "            sentence += vocab[index] # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "        else:\n",
    "            sentence.extend([vocab[UNK_IDX]])# 사전에 없는 인덱스면 unk 단어를 추가\n",
    "            \n",
    "        sentence += ' '  # 빈칸 추가\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a07e30",
   "metadata": {},
   "source": [
    "Seq2Seq에서는 학습시 다음과 같이 총 3개의 데이터가 필요하다..\n",
    "\n",
    "인코더 입력 : 질문<br>\n",
    "디코더 입력 : <sos> 답변 <br>\n",
    "디코더 출력 : 답변 <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "594554d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24240/3455674103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 인코더 입력 인덱스 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSent2Idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENCODER_INPUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mSent2Idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDECODER_INPUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24240/2114067942.py\u001b[0m in \u001b[0;36mSent2Idx\u001b[1;34m(sentences, vocabulary, type)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0msentence_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_sequences\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m         \u001b[1;31m# 최대 길이에 없는 공간은 패딩 인덱스로 채움\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0msentences_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_index\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# 문장의 인덱스 배열을 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences_index' is not defined"
     ]
    }
   ],
   "source": [
    "# 인코더 입력 인덱스 변환\n",
    "Sent2Idx(question, word_to_index, ENCODER_INPUT)\n",
    "Sent2Idx(answer, word_to_index, DECODER_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f624c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6701d36",
   "metadata": {},
   "source": [
    "# 모델 정의\n",
    "Seq2seq 모델은 가변 길이 시퀀스를 입력으로 받고, 크기가 고정된 모델을 이용하여, 가변 길이 시퀀스를 출력으로 반환하는 것을 목표로 한다.\n",
    "\n",
    "- Encoder(인코더): 가변 길이 입력 시퀀스를 고정 길이의 Context Vector로 인코딩\n",
    "- Context Vector(문맥 벡터): RNN의 마지막 은닉 레이어로 입력 시퀀스의 의미론적 정보\n",
    "- Decoder(디코더): 단어 하나, 문맥 벡터를 입력으로 받고, 시퀀스의 다음 단어가 무엇일지 추론한 결과와 단계에서 사용할 은닉 상태를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277519c",
   "metadata": {},
   "source": [
    "## 1. Encoder\n",
    "Encoder는 양방향 GRU(Bidirectional-Gated Recurrent Unit)를 사용하여 2개층으로 구성되어있다.입력 데이터의 Context 정보를 고정 길이의 벡터로 바꿔주는 역할을 하는데, 다시 말해 시퀀스의 요약된 정보를 Decoder로 전달하는 것이 목표라고 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "85a5ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers         # embedding: 입력값을 emd_dim 벡터로 변경\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)         # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # sre: [src_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))  # initial hidden state는 zero tensor\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # output: [src_len, batch_size, hid dim * n directions]\n",
    "        # hidden: [n layers * n directions, batch_size, hid dim]\n",
    "        # cell: [n layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f6448",
   "metadata": {},
   "source": [
    "## 파라미터 설명\n",
    "\n",
    "- input_dim = input 데이터의 vocab size = one-hot vector의 사이즈, 단어들의 index가 embedding 함수로 넘겨짐\n",
    "- emb_dim = embedding layer의 차원\n",
    "  (embedding 함수 : one-hot vector를 emb_dim 길이의 dense vector로 변환)\n",
    "- hid_dim = 은닉 상태의 차원 ( = cell state의 차원)\n",
    "- n_layers = RNN 안의 레이어 개수 (여기선 2개)\n",
    "- dropout = 사용할 드롭아웃의 양 (오버피팅 방지하는 정규화 방법)\n",
    "- n_directions = 1  (cf. bidirectional RNN의 경우 : n_directions=2)\n",
    "\n",
    "- 초기 은닉 상태, cell state 명시해주지 않으면, 디폴트로 모두 0으로 채워진 텐서로 초기화\n",
    "\n",
    "- outputs : 맨 위 레이어에서 각 time-stamp마다의 은닉 상태들\n",
    "\n",
    "- hidden : 각 레이어의 마지막 은닉상태, h_T\n",
    "\n",
    "- cell : 각 레이어의 마지막 cell state, c_T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b5e3e",
   "metadata": {},
   "source": [
    "## 2. 디코더(Decoder)\n",
    "\n",
    "Decoder는 Encoder가 전달한 Context Vector를 전달받아 결과를 도출한다. \n",
    "\n",
    "- Layer 1 : 직전 hidden state, cell state, embedded token 입력으로 받아 새로운 hidden stat와 cell state를 만들어냄 \n",
    "\n",
    "- Layer 2 : Layer 1의 은닉 상태, Layer 2에서 직전 hidden state, cell state를 입력fm로 받아 새로운 은닉 hidden state와 cell state를 만들어냄\n",
    "\n",
    "    - *Encoder Layer 1의 마지막 hidden state, cell state = context vector = Decoder Layer1 첫 hidden state, cell state*\n",
    "\n",
    "- Decoder RNN/LSTM의 맨 위 Layer의 은닉 상태를 Linear Layer에 넘겨서 다음 토큰을 예측함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d879f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # content vector를 입력받아 emb_dim 출력\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [n layers * n directions, batch_size, hid dim]\n",
    "        # cell: [n layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0) # input: [1, batch_size], 첫번째 input은 <SOS>\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input)) # [1, batch_size, emd dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output: [seq len, batch_size, hid dim * n directions]\n",
    "        # hidden: [n layers * n directions, batch size, hid dim]\n",
    "        # cell: [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0)) # [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fe1c4",
   "metadata": {},
   "source": [
    "## 파라미터 설명\n",
    "\n",
    "- output_dim = output 데이터의 vocab size (cf. input_dim은 데이터에서 주어진대로, output_dim은 우리가 직접 정해서 초기화)\n",
    "- hidden, cell은 각 time-stamp/각 레이어들의 은닉상태와 cell state들의 리스트\n",
    "- output은 마지막 time-stamp/마지막 레이어의 은닉상태만 \n",
    "- input = [batch size] → unsqueeze → input = [1, batch size]\n",
    "- input = [1, batch size] → embedding → dropout → embedded = [1, batch size, emb dim] \n",
    "- embedded, hidden, cell → rnn → output, hidden, cell\n",
    "\n",
    "- output = [1, batch size, hid dim] → squeeze → output = [batch size, hid dim]\n",
    "- output = [batch size, hid dim] → fc_out → prediction = [batch size, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b81e2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'Hidden dimensions of encoder decoder must be equal'\n",
    "        # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and decoder must have equal number of layers'\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src len, batch size]\n",
    "        # trg: [trg len, batch size]\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0] # 타겟 토큰 길이 얻기\n",
    "        trg_vocab_size = self.decoder.output_dim # context vector의 차원\n",
    "\n",
    "        # decoder의 output을 저장하기 위한 tensor\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # initial hidden state\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # 첫 번째 입력값 <sos> 토큰\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1,trg_len): # <eos> 제외하고 trg_len-1 만큼 반복\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # prediction 저장\n",
    "            outputs[t] = output\n",
    "\n",
    "            # teacher forcing을 사용할지, 말지 결정\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # 가장 높은 확률을 갖은 값 얻기\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # teacher forcing의 경우에 다음 lstm에 target token 입력\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3794c",
   "metadata": {},
   "source": [
    "원래 Seq2Seq는 디코더의 현재 출력이 디코더의 다음 입력으로 들어갑니다. 다만 학습에서는 굳이 이렇게 하지 않고 디코더 입력과 디코더 출력의 데이터를 각각 만듭니다. \n",
    "\n",
    "그러나 예측시에는 이런 방식이 불가능합니다. 출력값을 미리 알지 못하기 때문에, 디코더 입력을 사전에 생성할 수가 없습니다. 이런 문제를 해결하기 위해 훈련 모델과 예측 모델을 따로 구성해야 합니다. 모델 생성 부분에서 다시 자세히 설명을 드리겠습니다.????????????????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d2066",
   "metadata": {},
   "source": [
    "# 모델 학습 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdeb88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22c0a771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
