{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "packed-glance",
   "metadata": {},
   "source": [
    "# Seq2Seq 모델 구현 및 챗봇데이터 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot-torch clone\n",
    "!git clone https://github.com/Chat-with-U/chatbot-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attractive-statistics",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1633011128941,
     "user": {
      "displayName": "Moonjong Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13586540763833363239"
     },
     "user_tz": -540
    },
    "id": "8wjmEXQ-5kiX",
    "outputId": "60e8760f-1436-4827-ecbd-faef02f4ff74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (91/91), done.\n"
     ]
    }
   ],
   "source": [
    "#konlpy 설치\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electric-sender",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1633011128941,
     "user": {
      "displayName": "Moonjong Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13586540763833363239"
     },
     "user_tz": -540
    },
    "id": "8wjmEXQ-5kiX",
    "outputId": "60e8760f-1436-4827-ecbd-faef02f4ff74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (91/91), done.\n"
     ]
    }
   ],
   "source": [
    "# mecab 설치 프로그램 clone\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corresponding-norfolk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1633011128941,
     "user": {
      "displayName": "Moonjong Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13586540763833363239"
     },
     "user_tz": -540
    },
    "id": "eNQCVp-g3hIU",
    "outputId": "4c1004fd-eef1-4cd0-ca7f-2f933d01ee14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Mecab-ko-for-Google-Colab\n"
     ]
    }
   ],
   "source": [
    "cd Mecab-ko-for-Google-Colab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-essex",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119298,
     "status": "ok",
     "timestamp": 1633011248237,
     "user": {
      "displayName": "Moonjong Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13586540763833363239"
     },
     "user_tz": -540
    },
    "id": "PM2U8CtR5qOJ",
    "outputId": "9ccafc8b-efa2-4dfc-f990-8e59deab8d7a"
   },
   "outputs": [],
   "source": [
    "# mecab 설치 프로그램 실행\n",
    "!bash install_mecab-ko_on_colab_light_210108.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lovely-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import re\n",
    "import core.config as conf\n",
    "from konlpy.tag import Mecab  # tweepy오류로 konlpy 직접 설치 필요, mecab별도 설치 readme 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-congress",
   "metadata": {},
   "source": [
    "가상 환경 생성<br> \n",
    "`conda create -n 가상환경이름 python=3.7`\n",
    "\n",
    "PyTorch(Windows, Conda, CUDA 10.2)<br>\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functioning-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-aurora",
   "metadata": {},
   "source": [
    "# 데이터 전처리를 위한 설정\n",
    "Seq2Seq에서의 임베딩은 아래와 같이 추가 토큰을 사용하여 동작을 제어한다.\n",
    "\n",
    "- `<PAD>`: 0, Padding, 짧은 문장을 채울 때 사용하는 토큰\n",
    "- `<SOS>`: 1, Start of Sentence, 문장의 시작을 나타내는 토큰\n",
    "- `<EOS>`: 2, End of Sentence, 문장의 끝을 나타내는 토큰\n",
    "- `<UNK>`: 3, Unkown Words, 없는 단어를 나타내는 토큰\n",
    "\n",
    "디코더 입력에 <SOS>가 들어가면 디코딩(문장)의 시작을 의미하고 출력에 <EOS>가 나오면 디코딩(문장)을 종료한다.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-understanding",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "detected-picture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = conf.data_path\n",
    "chatbot_data = pd.read_csv(f'{path}'+'ChatbotData.csv')\n",
    "chatbot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "impressive-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 대화 쌍:  11823\n"
     ]
    }
   ],
   "source": [
    "print('총 대화 쌍: ', len(chatbot_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-technology",
   "metadata": {},
   "source": [
    "두 사람의 대화를 가정할 때 대화를 시작하는 대화를 question, 대답을 answer라 명명하겠습니다\n",
    "\n",
    "이 챗봇의 경우 모든 대화를 1턴으로 가정하기 때문에 [대화 시작, 대답]을 대화의 끝으로 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "restricted-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = chatbot_data.Q # Seq2Seq에 encoder_input\n",
    "answer = chatbot_data.A # Seq2Seq에 decoder_input\n",
    "total_utterances = pd.concat((question,answer)) #vocab을 만들기위한 전체 발화문장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-decrease",
   "metadata": {},
   "source": [
    "# 형태소분석\n",
    "\n",
    "챗봇 데이터 문장을 먼저 최소단위(우리말 형태소)로 tokenizing 해야합니다. 한국어는 KoNLPy라는 패키로 진행을 합니다.\n",
    "\n",
    "(해당 예시에서는 mecab 사용)\n",
    "(mecab외에도 Okt, komoran, kkma등 다른 형태소 분석기도 존재합니다. 각각 사용해보시고 차이점을 보는것도 재밌는 요소중 하나입니다)\n",
    "\n",
    "\n",
    "*최소단위(형태소)로 tokenizing한다는 이야기는 <span style=\"color:red\">형태소 단위로 문장을 쪼개는 것</span>을 의미합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quiet-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = Mecab() #konlpy의 대표적인 형태소 분석기 mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-sperm",
   "metadata": {},
   "source": [
    "단어 임베딩 vocab을 만들기위해 아래와같이 단어별로 나눈 뒤 <span style=\"color:red\">vocab 리스트</span>에 넣어줍니다.\n",
    "\n",
    "vocab 리스트는 갖고있는 모든 데이터셋의 단어 집합이라고 생각하시면 됩니다.\n",
    "\n",
    "예를들어 갖고있는 문장이 \n",
    "\n",
    "<span style=\"color:blue\">'내가 그린 기린 그림은 긴 기린 그림이고 니가 그린 기린 그림은 짧은 기린 그림이다'</span> 한문장이라고 생각한다면\n",
    "\n",
    "vocab list는 [내가, 그린, 기린, 그림, 은 ,긴, 이고, 니가, 짧은, 이다] 가 될 수 있습니다\n",
    "\n",
    "vocab에는 중복되는 단어를 최소화 해야하며 vocab은 추후 정수 인코딩 부분에도 등장하기 때문에 개념을 잘 기억하고 넘어가는게 중요합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "portuguese-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "\n",
    "special_tokens = ['[PAD]', '[MASK]', '[START]', '[END]', '[UNK]']\n",
    "for special_token in special_tokens:\n",
    "    vocab.append(special_token)\n",
    "\n",
    "for utterance in total_utterances:\n",
    "    for eojeols in pos_tagger.pos(utterance,flatten=False, join=True): # 대화를 형태소 분석기로 나눈 뒤 어절단위로 쪼갠 후\n",
    "        count = 0\n",
    "        for token in eojeols:\n",
    "            if count > 0:\n",
    "                if token in vocab:\n",
    "                    continue\n",
    "                vocab.append('##' + token) # 어절의 뒤에 나오는 형태소에 ##을 붙여 앞의 형태소와 이어지는 형태소임을 명시합니다. ex) 학교에 -> [학교, \"##에\"]\n",
    "            else:\n",
    "                if token in vocab:\n",
    "                    continue\n",
    "                vocab.append(token)\n",
    "                count += 1\n",
    "        \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fixed-warrior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8665"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-helmet",
   "metadata": {},
   "source": [
    "vocab_size는 만든 vocab이 몇개의 단어 혹은 형태소를 갖고있는가 나타내는 지표입니다.\n",
    "\n",
    "전체 11823개의 대화쌍의 데이터로 각 대화마다 2개의 문장이 존재한다고 하면 대략 2만4천개의 문장이 있다고 생각할 수 있습니다.\n",
    "\n",
    "즉 8665개의 단어/형태소를 이용하여 2만4천개의 문장을 표현할 수 있다는 것을 의미합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-cancer",
   "metadata": {},
   "source": [
    "# 토큰(단어/형태소) 인덱싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-quarterly",
   "metadata": {},
   "source": [
    "지금까지 형태소 분석기를 이용하여 우리가 갖고있는 데이터를 몇개의 형태소로 표현할 수 있는지 알아보았습니다.\n",
    "\n",
    "그렇다면 글자를 컴퓨터가 이해할 수 있는 숫자로 바꿔주는 작업이 필요합니다.\n",
    "\n",
    "그 첫번째 단계는 정수 인코딩 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-testament",
   "metadata": {},
   "source": [
    "정수 인코딩을 그림으로 표현하면 다음과 같습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/Chat-with-U/chatbot-study/blob/main/img/int_encoding.png?raw=true\"></p>\n",
    "\n",
    "vocab에는 단어에 맞는 index값이 있고 각 문장별로 단어를 vocab의 index로 매칭하는 과정을 거치게 됩니다.\n",
    "\n",
    "이는 추후 나오는 벡터화(vectorization)에서 다시한번 나오게 됩니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unnecessary-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2index = {token : index for index, token in enumerate(vocab)} # token(형태소)을 key, index를 value로 두어 특정 형태소를 index로 변환하는 테이블을 만듭니다.\n",
    "index2token = {index : token for index, token in enumerate(vocab)} # 추후 모델을 통해 추론된 정수 index를 token(형태소)으로 변환하는 테이블을 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-model",
   "metadata": {},
   "source": [
    "편의를 위해 WordHandler라는 클래스를 만들었습니다.\n",
    "\n",
    "클래스의 encode 부분은 문장을 vocab의 index로 변환하는 함수, decode는 인덱스를 문장으로 변환하는 과정을 수행하는 함수입니다.\n",
    "\n",
    "*decode without tag는 token뒤에 붙는 형태소 tag을 제거한 뒤 decode를 진행하는 함수입니다.\n",
    "\n",
    "하지만 max_seq_len의 역할은 무엇일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-missouri",
   "metadata": {},
   "source": [
    "## max sequence length(최대 문장 길이)\n",
    "\n",
    "cats & dogs, mnist 모델을 구현해보시거나 다른 이미지 분류 테스크를 진행해보신 분들이라면 image resize의 측면으로 접근이 가능합니다.\n",
    "\n",
    "image분류 task에서 resize를 왜 진행하게 될까요? \n",
    "\n",
    "image의 사이즈가 제각기 다르다면 마지막으로 얻어낸 feature vector의 크기가 달라지게 되고 last feature map을 flatten했을 때 제각기 다른 parameter size를 갖게 될것입니다.\n",
    "\n",
    "때문에 image의 사이즈를 통일하여 마지막 feature map의 사이즈를 통일시켜주게 됩니다.\n",
    "\n",
    "NLP에선 max_seq_len가 이와 같은 역할을 하게됩니다.\n",
    "\n",
    "문장의 길이가 전부 제각각이고 짧은 문장, 긴 문장을 하나의 문장 크기로 조정해주는 역할을 하게됩니다.\n",
    "\n",
    "긴 문장은 max_seq_len보다 길다면 조금 자르고 짧은 문장은 앞서 정의한 '[PAD]' 토큰으로 채워주어 각 문장의 길이를 맞추어 모델에 동일한 input을 넣을 수 있도록 조정하는 역할을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "severe-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordHandler:\n",
    "    def __init__(self, vocab, pos_tagger, token2index, index2token):\n",
    "        self.vocab = vocab\n",
    "        self.pos_tagger = pos_tagger\n",
    "        self.token2index = token2index\n",
    "        self.index2token = index2token\n",
    "        \n",
    "    def encode(self, sentence):\n",
    "        encoded_vector = [self.token2index[token] if token in self.token2index else self.token2index['[UNK]']\n",
    "                          for token in self.pos_tagger.pos(sentence, join= True)]\n",
    "        \n",
    "        return encoded_vector\n",
    "    \n",
    "    def decode(self, indice, join=True):\n",
    "        decoded_vector = [self.index2token[index] for index in indice]\n",
    "        \n",
    "        return decoded_vector\n",
    "    \n",
    "    def decode_without_tag(self, indice):\n",
    "        decoded_vector = ' '.join([self.index2token[index].split('/')[0] for index in indice])\n",
    "        \n",
    "        return decoded_vector\n",
    "    \n",
    "    @staticmethod\n",
    "    def return_max_seq_len(sentences):\n",
    "        max_seq_len = 0\n",
    "        for sentence in sentences:\n",
    "            max_seq_len = max(len(sentence), max_seq_len)\n",
    "        \n",
    "        return max_seq_len \n",
    "    \n",
    "    # 이 handler는 주어진 데이터셋에서 가장 긴 문장길이를 max_seq_len로 return합니다.\n",
    "    # 따라서 긴 길이의 문장이 잘려 손실이 발생하진 않지만 짧은 문장은 거의 [PAD]토큰으로 채워질 수 있습니다.\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-mediterranean",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Seq2Seq 모델로 데이터를 전달하기 위한 Dataset을 구현하였습니다.\n",
    "\n",
    "문장의 첫번째에 [START]토큰을 넣고 끝에 [END]토큰을 넣어 문장의 시작과 끝을 모델에 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "organized-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChitChatDataset(Dataset):\n",
    "    def __init__(self, input_ids, output_ids, index2token, token2index, max_seq_len):\n",
    "        self.input_ids = input_ids\n",
    "        self.output_ids = output_ids\n",
    "        self.index2token = index2token\n",
    "        self.token2index = token2index\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if len(self.input_ids[idx]) + 2 < self.max_seq_len:\n",
    "            padding_block = self.max_seq_len - len(self.input_ids[idx]) + 2\n",
    "            input = torch.LongTensor([self.token2index['[START]']] + \n",
    "                                     self.input_ids[idx] + \n",
    "                                     [self.token2index['[END]']] + \n",
    "                                     [self.token2index['[PAD]']] * padding_block)\n",
    "        else:\n",
    "            input = torch.LongTensor([self.token2index['[START]']] + \n",
    "                                     self.input_ids[idx] + \n",
    "                                     [self.token2index['[END]']])\n",
    "        \n",
    "        if len(self.output_ids[idx]) + 2 < self.max_seq_len:\n",
    "            padding_block = self.max_seq_len - len(self.output_ids[idx]) + 2\n",
    "            output = torch.LongTensor([self.token2index['[START]']] + \n",
    "                                     self.output_ids[idx] + \n",
    "                                     [self.token2index['[END]']] + \n",
    "                                     [self.token2index['[PAD]']] * padding_block)\n",
    "        else:\n",
    "            output = torch.LongTensor([self.token2index['[START]']] + \n",
    "                                     self.output_ids[idx] + \n",
    "                                     [self.token2index['[END]']])\n",
    "\n",
    "        return input, output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "extreme-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = WordHandler(vocab, pos_tagger, token2index, index2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-branch",
   "metadata": {},
   "source": [
    "모델에 들어갈 input과 output을 정의합니다.\n",
    "\n",
    "input은 대화의 시작, output은 그에대한 대답으로 이루어집니다.\n",
    "\n",
    "따라서 우리가 대화를 시작하면 챗봇은 그에 대한 대답을 하도록 학습되어지게 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "physical-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = question.map(handler.encode)\n",
    "output_ids = answer.map(handler.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "least-hardwood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 대화:  SNS 보 면 나 만 빼 고 다 행복 해 보여\n",
      "챗봇의 대답:  자랑 하 는 자리 니까요 .\n"
     ]
    }
   ],
   "source": [
    "print('사용자 대화: ', handler.decode_without_tag(input_ids[10]))\n",
    "print('챗봇의 대답: ', handler.decode_without_tag(output_ids[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "continuing-clinton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max(handler.return_max_seq_len(output_ids), handler.return_max_seq_len(input_ids))\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-macintosh",
   "metadata": {},
   "source": [
    "max_seq_len가 40이 나왔지만 조금 더 길게 대답하도록 만들어볼까요?\n",
    "\n",
    "max_seq_len를 60으로 설정해보겠습니다.\n",
    "\n",
    "물론 max_seq_len를 길게 설정한다고 모든 모델이 긴 대답을 하는 것은 아닙니다. 오히려 [PAD]토큰을 보는 빈도가 늘고 챗봇의 output이 [PAD]로 채우기만해도 loss가 떨어지는 현상이 발생할 가능성이 있습니다.\n",
    "\n",
    "하지만 길게 설정되면 어떤 결과를 가져올까요? 여러분이 직접 조절해가며 결과와 loss값을 확인하는것도 재밌는 요소중 하나가 될것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "indonesian-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset정의\n",
    "chitchat_data = ChitChatDataset(input_ids, output_ids, index2token, token2index, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "encouraging-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 정의\n",
    "chichat_dataloader = DataLoader(chitchat_data, batch_size= 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-permission",
   "metadata": {},
   "source": [
    "아래의 코드는 Seq2Seq모델을 정의하는 부분입니다.\n",
    "\n",
    "자세한 설명은 [링크]를 참조하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "subsequent-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers=1 ,batch_first=True)\n",
    "\n",
    "    def forward(self, encoder_embeds):\n",
    "        hidden_and_cell = self.lstm(encoder_embeds)[1]\n",
    "        return hidden_and_cell\n",
    "\n",
    "    \n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers=1 ,batch_first=True)\n",
    "\n",
    "    def forward(self, decoder_embeds, hidden_and_cell):\n",
    "        decoder_output = self.lstm(decoder_embeds, hidden_and_cell)[0]\n",
    "        return decoder_output\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = LSTMEncoder(embedding_dim)\n",
    "        self.decoder = LSTMDecoder(embedding_dim)\n",
    "        self.vocab_proj = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, encode_input, decode_input):\n",
    "        encoder_embeds = self.word_embedding(encode_input)\n",
    "        decoder_embeds = self.word_embedding(decode_input)\n",
    "        \n",
    "        hidden_and_cell = self.encoder(encoder_embeds)\n",
    "        decoder_output = self.decoder(decoder_embeds, hidden_and_cell)\n",
    "        projected_output = self.vocab_proj(decoder_output)\n",
    "        \n",
    "        return projected_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-chester",
   "metadata": {},
   "source": [
    "자 이제 학습을 위한 모든 준비가 끝났습니다. 과연 여러분의 챗봇은 어떤 대답을 하게될까요?\n",
    "\n",
    "직접 확인해보시죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(vocab_size, 512).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr)\n",
    "step = 0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    for encode_input, decode_input, target in tqdm(chitchat_dataloader):\n",
    "        encode_input = encode_input.cuda()\n",
    "        decode_input = decode_input.cuda()\n",
    "        target = target.cuda()\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        output = model(encode_input, decode_input)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(output.size()[0]):\n",
    "            loss += criterion(output[i][:len(target[i])], target[i])\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    sample_input = handler.decode_without_tag(encode_input[0].tolist())\n",
    "    sample_output = handler.decode_without_tag(output[0].argmax(-1).tolist())\n",
    "    print(f'loss: {loss}')\n",
    "    print(f'input_sentence: {sample_input}')\n",
    "    print(f'output_sentence: {sample_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
